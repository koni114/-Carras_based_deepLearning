< 케라스 창시자에게 배우는 딥러닝 책 간단 요약 >

기존의 딥러닝의 그래디언트를 전파하는 것이 가장 큰 문제였음
2009 ~ 2010년경 몇 가지 간단하지만 중요한 딥러닝 알고리즘이 개선되면서 그래디언트를 잘 전파하게 되고, 상황이 바뀜

- 신경망 층에 더 잘 맞는 활성화 함수(activation function)
- 층별 사전 훈련(pretraining)을 불필요하게 만든 가중치 초기화 방법
- RMSProp과 Adam 같은 더 좋은 최적화 방법

2014 ~ 2016년 사이에 그래디언트를 더욱 더 잘 전파할 수 있는 
batch normalization, residual connection, depthwise, separable, convolution 같은 고급 기술들이 개발 됨

** 딥러닝은 지속될까?
딥러닝이라는 자체가 20년 후에도 사용될지는 미지수지만, 비슷한 형태로 계속 사용될 것이라고 예측
why ? -> 크게 3가지 이유를 말하고 있음
- 단순함 : 딥러닝은 특성 공학이 필요하지 않아 불안정한 많은 엔지니어링 과정을 엔드 - 투 - 엔드로 훈련시킬 수 있는 모델로 바꾸어 줌
- 확장성 : GPU 또는 TPU에서 쉽게 병렬화 할 수 있음
- 다용도와 재사용성 :  딥러닝 모델은 처음부터 다시 모델링 하지 않고, 추가되는 데이터로도 훈련할 수 있음
		    딥러닝 모델은 다른 용도로 사용될 수 있어,  재사용이 가능

## ------------------------------------------------------------------------ ##

< chapter 2 시작하기 전에 : 신경망의 수학적 구성 요소>
손글씨 숫자 분류를 학습하는 예제를 통해 신경망의 수학적 구성 요소를 이해해보자! 
문제 정의 ->  흑백 손글씨 숫자 이미지(28 X 28 픽셀)를 10개의 범주(0~9까지)로 분류하는 것
 
분류 문제의 범주 -> 클래스
데이터 포인트    -> 샘플
특정 샘플의 클래스 -> 레이블(Label)
 
작업순서
1. 훈련데이터를 네트워크에 주입
2. 네트워크는 이미지와 레이블을 연관시킬 수 있도록 학습
3. 마지막으로 test_image에 대한 예측을 네트워크에 요청
4. 요청된 값과 실제 값과 비교

신경망의 핵심 요소 -> 층(layer)
층은 주어진 문제에 더 의미 있는 *표현(representation)을 입력된 데이터로부터 추출 함

신경망이 훈련 준비를 마치기 위해서 컴파일 단계에 포함될 세 가지 필요
- 손실 함수(loss function) : 훈련 데이터에서 신경망의 성능을 측정하는 방법으로 네트워크가 옳은 방향으로 학습될 수 있도록 도와 줌
- 옵티마이저(optimizer)    : 입력된 데이터와 손실 함수를 기반으로 네트워크를 업데이트하는 매커니즘
- 평가지표(metrics)         : 훈련과 테스트 과정을 모니터링 할 지표 

신경망은 입력 데이터의 스케일에 민감하기 때문에, 스케일링 과정 필요.
이미지의 경우 보통 픽셀의 최댓값인 255로 나누어 사용
레이블을 범주형으로 인코딩 해야 함( one-hot encoding 해야 된다는 말인 듯)
ex ) [0, 2] -> [[1,0,0],[0,0,1]]

훈련시 2개의 정보 출력 : 손실(loss value) 과 정확도(accuracy)

** 텐서(tensor)란 ? 
머신러닝의 기본 구성 요소
데이터를 위한 컨테이너(container). 거의 항상 수치형 데이터를 다루므로 숫자를 위한 컨테이너
임의의 차원 개수를 가지는 행렬의 일반화된 모습(여기서 차원(dimension)을 종종 축(axis)이라고 부름)

** 스칼라(Scalar, 0D 텐서)
하나의 숫자만 가지고 있는 텐서
스칼라 축의 개수는 0. 보통 텐서의 축 개수를 rank라고 함

** 벡터(Vector, 1D 텐서)
숫자의 배열을 벡터라고 함. 또는 1D 텐서!
딱 하나의 축을 가짐
벡터가 5개의 원소를 가지고 있는 경우,  5차원 벡터라고 함!(중요)
** 주의! 5D 텐서와 5D 벡터와 혼동하지 말자~
여기서 차원수(dimensionality)는 특정 축을 따라 놓인 원소의 개수이거나, 
텐서의 축 개수를 의미하므로 혼동하기 쉬움

** 행렬(matrix, 2D 텐서)
벡터의 행렬(matrix) 또는 2D 텐서라고 함
2개의 축 존재(보통 행과 열이라고 부름)

** 3D 텐서와 고차원 텐서 
행렬들을 하나의 새로운 배열로 합치면 숫자가 채워진 
직육면체 형태로 해석할 수 있는 3D 텐서가 만들어짐.

딥러닝에서는 보통 0D ~ 4D까지의 텐서를 다룸
동영상 같은 경우, 5D까지 가기도 함

** 텐서의 핵심 속성
- 축의 개수(랭크) : 축은 앞에서 이미 배웠음, numpy library 의 ndim 속성에 저장.
- 크기(shape) : 각 축에서의 차원을 의미. 즉 tuple의 길이!
- 데이터의 타입 : 텐서에 포함된 데이터 타입(numpy에서는 dtype에 저장)
	          일반적으로 텐서의 타입은 float32, uint8(8bit 정수형 의미), float64 등,, 간혹가다 char 타입 있음

텐서는 사전에 할당되어 연속된 메모리에 저장되어야 하므로 
넘파이 배열은 가변 길이의 문자열을 지원하지 않음(?)

넘파이로 텐서 조작하기
정중앙에 위치한 14 X 14 픽셀 조각을 이미지에서 잘라 내려면 ..?
train_images[: , 7:-7, 7:-7]

** 배치 데이터
일반적으로 딥러닝에서 사용하는 모든 데이터 텐서의 첫 번째 축(axis = rank)은 샘플 축(sample axis) 임
-> 여기서 샘플 축은 데이터의 개수(ex 이미지의 개수) 등을 의미하는 데이터의 수를 말함
일반적으로 딥러닝 모델은 한꺼번에 전체 데이터 셋을 처리하지 않고, 데이터를 작은 배치로 나눔
보통 첫번 째 축(= 샘플 축)을 배치 축(batch axis) 또는 배치 차원(batch dimension)이라고 부름

** 텐서의 실제 사례
우리가 사용할 데이터는 대부분 다음 중 하나에 속함!
벡터 데이터                                            : (samples, features) 크기의 2D 텐서
시계열 데이터 또는 시퀀스(sequence) 데이터 : (samples, timesteps, features) 크기의 3D 텐서
이미지 데이터                                         : (samples, height, width, channels) 또는 (samples, channels, height, width) 크기의 4D 텐서
동영상 데이터                                         : (samples, frames, height, width, channels) 또는 (samples, frames, channels, height, width) 크기의 5D 텐서

* 벡터 데이터
대부분의 경우에 해당!
이런 데이터셋에서는 하나의 데이터 포인트가 벡터로 인코딩될 수 있으므로 배치 데이터는 2D 텐서로 인코딩 됨
첫번째 축은 샘플 축(sample axis), 두번째 축은 특성 축(feature axis) 임
2개의 예
1. 사람의 나이, 우편 번호, 소득으로 구성된 10만명 인구 통계 데이터 -> (100000, 3) 크기의 텐서에 저장 됨
2. 공통 단어 2만 개로 만든 사전에서 각 단어가 등장한 횟수로 표현된 텍스트 문서 500개 -> (500, 20000) 크기의 텐서에 저장 됨

* 시계열 데이터 또는 시퀀스 데이터
시간이 중요할 때는 시간 축을 추가한 3D 텐서에 저장 
2개의 예
1. 주식 가격 데이터 셋 : 1분마다 (주식 가격, 지난 1분 동안 최고 가격, 최소 가격) 3개의 스칼라를 저장
 		      즉, 1분마다 데이터는 3차원 벡터로 인코딩 되고, 하루 동안 거래는 (390, 3) 크기의 2D 텐서로 저장
                               250일치의 데이터는 (250, 390, 3) 크기의 3D 텐서로 저장
 
2. 트윗 데이터 셋 : 각 트윗은 128개의 알파벳으로 구성된 280개 문자 시퀀스 -> (280, 128) 
                        100개의 트윗으로 구성된 데이터셋은 (1000000, 280, 128) 크기의 텐서에 저장
	            -> 한 행이 단어 하나가 됨.

* 이미지 데이터 
전형적으로 높이(세로), 너비(가로), 컬러 채널의 3차원으로 이루어짐
컬러 이미지에 대한 128개의 배치 -> (128, 256, 256, 3) 으로 이루어짐! 헷갈리지 말자..
이미지 텐서의 크기를 지정하는 방식은 두가지
- 텐서플로우에서 사용하는 channel-last 방식 -> (samples, height, width, color_depth)
- 씨아노에서 사용하는 channel-first 방식      -> (samples, color_depth, height, width)
케라스 프레임워크는 두 방식 모두 지원!

* 비디오 데이터
비디오 <- 여러 프레임 <- 여러 컬러 이미지 임!
결과적으로, (samples, frames, height, width, color_depth) 인 5D 텐서에 저장할 수 있음
1개의 예
60초짜리 144 X 256 유투브 비디오 클립을 초당 4프레임으로 샘플링 하는 경우, -> 240 프레임
이런 비디오가 4개 있다고 가정할 때,
-> (4, 240, 144, 256, 3) 크기의 텐서에 저장될 수 있음
총 106168320개의 값이 있는데, 이 텐서를 dtype을 float32로 했다면, 각 값이 32bit로 저장될 것이므로,  

## ------------------------------------------------------------------------ ##

< 소주제 - 신경망의 톱니바퀴 : 텐서 연산 >
심층 신경망이 학습한 모든 변환을 수치 데이터 텐서에 적용하는 몇 종류의 텐서 연산으로 나타 낼 수 있음
Dense 층을 쌓아서 만듬. 케라스 층은 다음과 같이 생성할 수 있음

Keras.layers.Dense(512, activation ='relu')
-> 이 층은 2D텐서를 입력으로 받고 입력 텐서의 새로운 표현인 또 다른 2D 텐서를 반환하는 함수처럼 해석 가능

output = relu(dot(W, input) + b)
다음은 3가지 연산이 들어가 있음
- W와 input 텐서와의 곱
- 해당 결과 텐서와 b와의 덧셈 연산
- 이를 relu 연산으로 수행

** 원소별 연산(element-wise operation)
relu 연산, 더하기 연산에 해당
텐서에 있는 각 연소에 독립적으로 적용
원소별 연산은 넘파이에서 빠른 속도로 해결 가능

** 브로드캐스팅(broadcasting)
작은 텐서가 큰 텐서의 크기에 맞추어 지는 것
브로드캐스팅은 두 단계로 이루어 짐
1. 큰 텐서의 ndim에 맞도록 작은 텐서에 축이 추가
2. 작은 텐서가 새 축을 따라서 큰 텐서의 크기에 맞도록 반복

** 텐서 점곱(tensor product)
입력 텐서의 원소들을 결합 -> (연산으로 텐서가 작아진다는 말!)
넘파이, 케라스, 씨아노, 텐서플로에서 원소별 곱셈은 * 연산자를 사용하고,
텐서 점곱은 dot 연산자가 다르지만, 보편적인 dot 연산자를 사용
ndim이 1보다 큰 경우, 교환 법칙은 성립하지 않음

** 텐서 크기 변환(tensor reshaping)
특정 크기에 맞게 열과 행을 재배열한다는 뜻
자주 사용하는 크기 변환은 전치(transposition) -> tensor[i, ] <-> tensor[,i]

** 텐서 연산의 기하학적 해석
텐서 연산이 조작하는 텐서의 내용은 기하학적 공간에 있는 좌표 포인트로 해석이 가능
일반적으로 아핀 변환(affine transformation), 회전, 스케일링 등 기본적인 기하학적 연산은 텐서 연산으로 표현 될 수 있음
예를 들어 theta 회전은 2X2 행렬 R = [u, v]를 점곱하여 구현할 수 있음
-> u = [ cos(theta), sin(theta) ], v = [ -sin(theta), cos(theta) ] 

** 딥러닝의 기하학적 해석
꾸겨 진 종이공을 펼치는 것이 머신 러닝이 하는 일
심하게 꼬여 있는 데이터의 매니폴드에 대한 깔끔한 표현을 찾는 일
딥러닝 -> 기초적인 연산을 길게 연결하여 복잡한 기하학적 변환을 조금씩 분해하는 방식이므로, 
	 꾸겨진 종이공을 넓게 색종이로 펼치는 작업에 유능

* 매니폴드 : 저차원의 유클리디안 거리로 볼 수 있는 고차원 공간(ex) 2차원 색종이로 볼 수 있는 꾸겨진 3차원 종이 공)

## ------------------------------------------------------------------------ ##

< 소주제 : 신경망의 엔진 : 그레디언트 기반 최적화 >
다음의 식을 다시 상기해보자
output = relu(dot(W, input) + b)
텐서 W와 b는 층의 속성처럼 볼 수 있음 -> 가중치(weight) 또는 훈련되는 파라미터(trainable parameter)라고 부름
                                                       -> 또는 커널(kernel), 편향(bias) 라고 부름

초기에는 가중치 행렬이 무작위 난수로 만들어져 있음(random initialization phase)
피드백 신호에 의해 점진적으로 조정 됨. 이 훈련이 머신 러닝 학습의 핵심

훈련 반복 루프(loop)
1. 훈련 샘플 x와 이에 상응하는 y의 배치 추출
2. x를 사용하여 네트워크 실행 및 y_pred 를 구함
3. y_pred와 y의 차이를 측정하여 배치에 대한 손실 계산
4. 배치에 대한 손실이 조금 감소되도록 네트워크의 모든 가중치 업데이트

** 핵심은 네트워크의 모든 가중치 업데이트!
How? 가장 쉬운 방법은 모든 가중치를 고정하고 관심 있는 하나만 바꿔가면서 손실 함수의 value check
-> 이 방법은 모든 가중치 행렬의 원소마다 두 번의 정방향 패스를 계산해야 하므로 비효율적
- 신경망에 사용된 모든 연산이 미분 가능하다는 장점을 사용하여, 네트워크 가중치에 대한 손실의 gradient를 계산하는 것이 훨씬 좋은 방법

** 텐서 연산의 변화율:  그래디언트(gradient)
그래디언트는 텐서 변환의 변화율 -> 텐서를 입력으로 받는 함수에 변화율 개념을 확장시킨 것.

W를 사용하여 타깃의 예측 y_pred를 계산하고 손실, 즉 타깃 예측 y_pred와 타깃 y 사이의 오차 계산할 수 있음
loss_value = loss(y_pred, y)

x,y가 고정일 때, 이 함수는 W를 손실 값에 매핑하는 함수로 볼 수 있음
loss_value = f(W)
W의 현재 값을 W0라고 할 때, 포인트 W0에서 f의 변화율  
-> W와 같은 크기의 텐서인 gradient(f)(W0)
** 이 텐서의 각 원소 gradient(f)(W0)[i, j] 는 W0[i, j]를 변경했을 때 loss_vale가 바뀌는 방향과 크기를 나타냄
gradient(f)(W0) -> W0에서 함수 f(W) = loss_value의 그래디언트
                    -> W0에서 f(W)의 기울기를 나타내는 텐서로 해석

즉, 그래디언트의 반대 방향으로 W를 움직이면, f(W)의 값을 줄일 수 있음
ex) W1 = W0 - step * gradient(f)(W0) (step은 scale을 조정하기 위한 작은 값)

** 확률적 경사 하강법(gradient descent algorithm)
가장 작은 손실 함수의 값을 만드는 가중치의 조합을 해석적으로 찾는 것을 의미
gradient(f)(W) = 0을 풀면 해결. 이는 N(네트워크 가중치 개수)개의 변수로 이루어진 다항식!
신경망에서는 N이 수백개, 수천개 이므로, 이를 해석적으로 푸는 것은 불가

따라서 앞서 가중치 업데이트 아이디어를 적용,
-> 랜덤한 배치 데이터에서 현재 손실 값을 토대로 하여 조금씩 파라미터를 수정하는 것!
    그래디언트의 반대 방향으로 가중치를 업데이트 하면 손실이 매번 조금씩 감소 할 것임 ..

1. 훈련 샘플 배치 x와 이에 상응하는 타깃 y를 추출
2, x로 네트워크를 실행하고 예측 y_pred를 구함
3. y_pred와 y의 손실 값 계산
4. 손실 함수의 gradient를(역방향 Pass..) 계산
5. gradient의 반대방향으로 step 값을 곱해 조금씩 값을 감소
-> 이 방법이 미니 배치 확률적 경사 하강법(mini-batch stochastic gradient descent = 미니 배치 GSD)에 해당
   왜 확률적이냐 ? --> 배치 X를 무작위로 추출하기 때문! 보통 확률적이다 라고 하는 것은 무작위의 과학적 표현에 해당

step 값을 적절하게 setting 하는 것이 중요. 너무 크면 손실 함수 곡선에서 완전히 다른 위치로 바뀜.
너무 작으면, local minimum에 갇힘

배치 SGD의 변종(?) 중 하나는 배치 선택 방법에 있음
즉, 반복마다 하나의 샘플과 하나의 타깃을 뽑는 것일 수도 있고(원래 이게 진정한 SGD),
극단적으로 모든 X 데이터를 배치 데이터로 setting해 반복을 수행 할 수도 있음(배치 SGD)

실제로는 굉장히 고차원 공간에서 경사 하강법을 사용하게 됨 ( 보통 설명할때는 1D 파라미터 공간에서 많이 하지만 .. 헷갈리지 말자!)
2D 손실 함수의 표면을 따라 진행하는 경사 하강법을 시각화해 볼 수 있지만,
딥러닝은 10000차원 이상을 표현해야 하는데, 이를 공간에 표현하는 것은 불가능.
 --> ** 즉, 저차원 표현으로 얻은 직관이 실전과 항상 맞지는 않다는 것을 유념하자!. 이는 딥러닝 분야에서 여러 이슈를 일으키는 근원,
 어떤 이슈? ex) 신경망 알고리즘이 local minimum에 쉽게 접근 할 것이라 생각했지만, 고차원 공간에서는 안장점(saddle point)으로 나타나고
                     지역 최소값은 매우 드뭄. 수치적으로 보면 1000개의 파라미터가 있는 공간에서 변화율이 0인 지점의 모든 파라미터가 최솟값일 가능성은
                     2 ^(-1000)

이런 가중치를 계산할 때 여러 변종 들, 즉 여러 방법들을 적용해서 계산하는데, 
이런 변종들을 모두 최적화 방법(optimization method) 또는 옵티마이저(optimizer)라고 함.

** 모멘텀(momentum) 알고리즘
과거 그래디언트가 가지고 있는 어떤 방향을 현재 그래디언트에 보정하려는 방식
이런 모멘텀은 SGD에 있는 2가지 문제점, 수렴 속도와 지역 최소값을 해결
-> 지역 최소값에 도달했을때 왼쪽, 오른쪽으로 이동하려고 해도 손실함수는 증가하므로
    결과적으로 갇히게 된다. -> 문제 발생
이를 어떻게 해결할까? 손실 함수 위에 축구공을 굴리는 것을 생각해보자. 
국소 최소값에 도착했을 때, 축구공이 힘을 받아 국소 최소값을 벗어날 힘을 가지고 있다고 하면 
전역 최소값에 도달할 수 있게 된다. (이해했지?)
즉, 모멘텀은 현재 기울기 값(현재 가속도)뿐만 아니라 현재 속도를 함께 고려하여 각 단계에서 공을 움직인다


** 변화율 연결: 역전파 알고리즘
3개의 텐서 연산 a, b, c와 가중치 행렬 W1, W2, W3로 구성된 네트워크 f를 예를 들어보자
f(W1, W2, W3) = a(W1, b(W2, c(W3))) -> 즉, 단계적으로 변환을 3번 한다는 얘기.

이러한 함수 내에 함수가 들어가 있는 함수를 미분할 때, chain Rule를 적용하여 풀어 낼 수 있음 ( f(g(x))' = f'(g(x)) * g'(x) )
연쇄 법칙을 신경망의 그래디언트 계산에 적용하여 역전파 알고리즘이 탄생함!
역전파는 최종 손실 값에서부터 시작. 
손실 값에 각 파라미터가 기여한 정도를 계산하기 위해 연쇄 법칙을 이용하여 최상위 층에서 최하위 층까지 거꾸로 진행

(?) 향후 몇 년 동안은 텐서플로처럼 기호 미분(symbolic differentiation)이 가능한 최신 프레임워크를 사용하여 신경망 구현할 것
-> 변화율이 알려진 연산(+, -, x, %...등)들로 연결되어 있으면 네트워크 파라미터와 그래디언트 값을 매핑하는 그래디언트 함수를 계산할 수 있다는 의미

결과적으로 정확한 역전파 공식을 구현하지 않아도 되고, 어떻게 돌아가는지 이해만 하면 됨 







 
















