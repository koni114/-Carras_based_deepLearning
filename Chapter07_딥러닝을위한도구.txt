
<Chapter07 딥러닝을 위한 도구>
여기서 알 수 있는 것.
- 케라스의 함수형 API 사용하여 그래프 구조를 띤 모델 만들기
- 하나의 층을 다른 입력에 같이 사용
- 케라스 모델을 파이썬 함수처럼 사용
- 케라스 callback
- 텐서보드 사용
- 배치 정규화, 잔차 연결, 하이퍼파라미터 최적화, 모델 앙상블을 통한 모범 사례

7.1 Sequential 모델을 넘어서: 케라스의 함수형 API

Sequential 모델은 네트워크 입력과 출력이 하나라고 가정.
but, 이런 가정이 맞지 않는 경우도 많음.
ex) 개별 입력이 여러 개 필요하거나 출력이 여러 개 필요
     층을 차례대로 쌓지 않고 층 사이를 연결하여 그래프처럼 만드는 경우.

예시 1   
중고 의류 시장 가격을 예측하는 딥러닝 모델(입력 값이 여러 개)
input data -> 메타데이터(의류 브랜드, 연도 등), 사용자가 제공하는 text 설명, 제품 사진
각 데이터를 완전 연결 모듈, RNN 모듈, 컨브넷 모듈에 연결하여 가중 평균을 한다!
-> 이를 다중 입력 모델이라고 함.

예시 2
소설이나 짧은 글이 있을 때 자동으로 장르별 구분
글을 쓴 대략의 시대 예측
2개의 모델을 따로 훈련할 수 있지만 이 속성들은 통계적으로 독립적이지 않기 때문에 
동시에 장르와 시대를 함께 예측하도록 해야 더 좋은 모델을 만들 수 있음.
why ? 두 개의 예측 값은 상관관계 때문에 소설 시대를 알면, 장르의 공간에서 정확하고 풍부한
         표현을 학습하는데 도움을 줌.

최근 개발된 신경망 구조는 선형적이지 않은 network topology가 필요
ex) 인셉션 모듈을 사용하는 인셉션 개열의 네트워크
     -> 이 모듈에서 입력은 나란히 놓인 여러개의 합성곱 층을 거쳐 하나의 텐서로 출력이 합쳐짐

** network topology(위상 수학(?))
비순환 유향 그래프 같은 네트워크 구조

최근에는 모델에 잔차 연결을 추가하는 경향도 있음
ResNet 계열의 네트워크들이 이런 방식을 사용하기 시작.
텐서를 상위 층의 출력 텐서에 더해서 아래층의 표현이 네트워크 위쪽으로 흘러갈 수 있도록 함(아래 층 값이 윗 층의 값에 영향을 미치는 구조)
-> 하위 층에서 학습된 정보가 데이터 처리 과정에서 손실되는 것을 방지

결과적으로 다중 입력 모델, 다중 출력 모델, 그래프 구조를 띤 모델이 필요하지만
케라스의 Sequential 클래스를 사용해서는 만들지 못함.

** 7.1.1 함수형 API 소개
함수처럼 층을 사용하여 텐서를 입력받고 출력함

input_tensor = Input(shape = (32, )) 
dense = layers.Dense(32, activation = 'relu')
output_tensor = dense(input_tensor)

Model 객체를 사용한 컴파일, 훈련, 평가 API는 Sequential 클래스와 같음

** 7.1.2 다중 입력 모델
함수형 API는 다중 입력 모델을 만드는 데 사용할 수 있음
서로 다른 입력 가지를 합치기 위해 여러 텐서를 연결할 수 있는 층을 사용
ex) 텐서를 더하거나, 이어 붙이는 방법.
keras.layers.add / keras.layers.concatenate 등

* 간단한 다중 입력 모델
ex) 질문 응답 모델
 
참고 텍스트 -> Embedding -> LSTM
				-> Concatenate -> Dense -> 응답
질문           -> Embedding -> LSTM 

* Embedding
범주형 자료를 연속형 벡터 형태로 변환시키는 것을 embedding이라고 함.

입력이 2개인 모델의 훈련 방법은 2가지
- 넘파이 배열의 리스트를 주입
ex) model.fit({[text, question], answers, epochs = 10, batch_size = 128)

- 입력 이름과 넘파이 배열로 이루어진 딕셔너리를 모델의 입력으로 주입할 수 있음
ex) model.fit({'text' : text, 'question': question}, answers, epochs=10, batch_size = 128)

7.1.3 다중 출력 모델
함수형 API를 사용하여 다중 출력 모델을 만들 수 있음
데이터에 있는 여러 속성을 동시에 예측하는 네트워크
ex) 소셜 미디어에서 익명 사용자의 포스트를 입력으로 받아 그 사람의 나이,
     성별, 소득 수준 등을 예측

소셜 미디어 포스트 -> 1D 컨브넷  -> Dense  -> 나이
			       -> Dense  -> 소득
  			       -> Dense  -> 성별
 

네트워크 출력마다 다른 손실함수를 지정해 주어야 함 
ex) 나이 예측은 스칼라 회귀 문제 이지만, 성별 예측은 이진 클래스 문제라 훈련 방식이 다름
경사 하강법은 하나의 스칼라 값을 최소화하기 때문에 모델을 훈련하려면 이 손실들을
하나의 값으로 합쳐야 함
-> 가장 간단한 방법은 모두 더하는 방법

케라스에서는 compile method에 리스트나 딕셔너리를 사용하여
출력마다 다른 손실을 지정할 수 있음.
-> 계산된 손실 값은 전체 손실 하나로 더해지고 훈련 과정을 통해 최소화 됨

model.compile(optimizer = 'rmsprop',
	        loss = ['mse', 'categorical_crossentropy', 'binary_crossentropy'])

손실 값이 많이 불균형하면 모델이 개별 손실이 가장 큰 작업에 치우쳐 
표현을 최적화함 -> 다른 작업들은 손해를 입을 수 있음

손실 값이 최종 손실에 기여하는 수준을 지정할 수 있음
ex) 손실 값의 스케일이 다를 때 유용
     나이 회귀 작업에 사용되는 MSE 손실은 3~5사이를 가지고 
     성별 분류 작업에 사용되는 크로스엔트로피 손실은 0.1 정도로 낮을 때,

    각각 가중치를 통해 균형을 맞춰 줄 수 있음!

model.compile(optimizer = 'rmsprop',
	        loss = ['mse', 'categorical_crossentropy', 'binary_crossentropy'],
	        loss_weights = {'age' = 0.25, 'income' : 1., 'gender' : 10.} 
)

7.1.4 층으로 구성된 비순환 유향 그래프
함수형 API를 사용하면, 내부 토폴로지가 복잡한 네트워크도 만들 수 있음
케라스의 신경망은 층으로 구성된 어떤 비순환 유향 그래프도 만들 수 있음

** 인셉션 모듈(Inception Module)
인셉션은 합성곱 신경망에서 인기 있는 네트워크 구조.
네트워크 안의 네트워크 구조에서 영감을 받아 2013~2014년에 크리스티안 세게디와 그의 구글 동료들이 만듬.
가장 기본적인 인셉션 모듈 형태는 3~4개의 가지를 가짐
1X1 합성곱으로 시작해서 3X3 합성곱이 뒤따르고 마지막에 전체 출력 특성이 합쳐짐
이런 구성은 네트워크가 따로따로 공간 특성과 채널 방향의 특성을 학습하도록 도움
(각 네트워크 채널마다 특유의 특성을 학습한다는 의미)
-> 한꺼번에 학습하는 것 보다 효과가 높음

더 복잡한 인샙션 모듈은 풀링 연산, 여러 가지 합성곱 사이즈, 공간 합성곱이 없는 가지로 구성될 수 있음.

** 인셉션V3  

		Conv2D, 1X1 strides = 2                                            --> 

		Conv2D 1X1   -->  Conv2D 3X3 strides = 2                   -->
입력 ->                                                                                                         Concatenate --> 출력
		avgPool2D 3X3 strides = 2 --> Conv2D 3X3                   --> 

		Conv2D 1X1 --> Conv2D 3X3 --> Conv2D 3X3 strides =2 -->


** 1X1 합성곱의 목적
추출된 패치가 하나의 타일로 이루어졌을 때.
이 합성곱 연산은 모든 타일의 벡터를 하나의 Dense 층에 통과시키는 것과 동일
입력 텐서의 채널 정보를 혼합한 특성을 계산. 공간 방향으로는 정보를 섞지 않음. 
1X1은 인셉션 모듈의 특징.
채널 방향의 특성 학습과 공간 방향의 특성 학습을 분리하는데 도움을 줌.
채널이 공간 방향으로 상관관계가 크고 채널 간에는 독립적이라고 가정하면 납득할 만한 전략

keras.applications.inception_v3.InceptionV3

** Xception
극단적인 inception을 말함.
채널 방향의 학습과 공간 방향의 학습을 극단적으로 분리한다는 
아이디어에 착안하여 인셉션 모듈을 깊이별 분리 합성곱으로 바꿈.
합성곱은 깊이별 합성곱(각 입력 채널에 따로따로 적용되는 공간 방향 합성곱**) 다음에 점별 합성곱(1X1)이 뒤따름.
인셉션 모델의 극한 형태로, 공간 특성과 채널 방향 특성을 완전히 분리.

엑셉션 모델은 3X3 커널을 사용한 layers.SeperableConv2D 합성곱 사용
입력 채널에 대해 따로따로 3X3 합성곱을 수행(채널을 늘리지 않음). 그다음 1X1 점곱을 수행(출력 채널을 늘림)
 SeparableConv2D의 출력 채널은 128개에서 2048개까지 네트워크가 깊어질수록 늘어남.
keras.applications.xception.Xception에 포함되어 있음.

인셉션V3 모델과 거의 동일한 파라미터 개수를 가지지만, 
실행 속도가 더 빠르고 ImageNet 이나  다른 대규모 데이터셋에서 정확도가 더 높음.
-> 모델 파라미터를 좀 더 효율적으로 사용하기 때문.
	익셉션 네트워크 파라미터 수 : 2380만 개
	엑셉션 네트워크 파라미터 수 : 2280개

** 잔차연결(residual connection)
2015년 이후 등장한 많은 네트워크 구조에 있는 그래프 형태의 네트워크 컴포넌트.
대규모 딥러닝 모델에서 흔히 나타나는 두 가지 문제인 그래디언트 소실과 표현 병목을 해결.
일반적으로 10개 이상을 가진 모델에 잔차 연결을 추가하면 도움이 됨.

잔차 연결은 하위 층의 출력을 상위 층의 입력으로 사용
순서대로 놓인 네트워크를 질러가는 연결이 만들어짐.
하위 층의 출력이 상위 층의 활성화 출력에 연결되는 것이 아니라, 더해 짐.
-> 두 출력의 크기가 동일해야 함.

크기가 다르면 선형 변환을 사용하여 하위 층의 활성화 출력을 목표 크기로 변환함.
출력 특성의 크기(채널 수)가 다를 수 있다. 이를 맞춰 주는 것. 
ex) 선형 변환 예 
     활성화 함수를 사용하지 않는 Dense 층.
     합성곱의 특성 맵이라면, 활성하 함수가 없는 1X1 합성곱.

** 딥러닝의 표현 병목
Sequential한 모델에서 해당 층은 이전 층의 활성화 출력 정보만 사용.
어떤 층이 너무 작으면, 이 활성화 출력에 얼마나 많은 정보를 채울 수 있느냐에 모델 성능이 좌우.
이러한 문제를 residual connection은 어느정도 해결해 줌.

** 그래디언트 소실 문제.
핵심 알고리즘인 역전파는 출력 손실에서 얻은 피드백 신호를 하위 층에 전파.
but, 피드백 신호가 깊이 쌓인 층을 통과하면서 신호가 아주 작아지거나, 완전히 사라질 수 있음.
이를 그래디언트 소실(vanishing gradient) 문제라고 함.

이 문제는 심층 신경망과 긴 시퀀스를 처리하는 순환 신경망에서 모두 나타남.
주 네트워크 층에 나란히 단순한 선형 정보를 실어 나름.
-> 이는 그래디언트가 깊게 쌓인 층을 통과하여 전파하도록 도움.

잔차 연결에서 하위 층의 출력과 상위 층의 출력을 단순히 더했으므로 그래디언트가 그대로 잔차 연결을 따라
하위 층으로도 전달 됨.
잔차 연결을 따라 내려온 그래디언트는 주 네트워크 층을 거쳐 줄어든 그래디언트와 더해짐.

7.1.5 층 가중치 공유
함수형 API의 중요한 기능 -> 층 객체를 여러 번 재사용 가능!
즉, 층 객체를 두 번 호출하면 새로운 층 객체를 만들지 않고 각 호출에 동일한 가중치를 재사용.

공유 가지를 가지는 모델을 만들 수 있음.
이런 가지는 같은 가중치를 공유하고, 같은 연산을 수행.
-> 같은 표현을 공유하고 이런 표현을 다른 입력에서 함께 학습.

ex) 두 문장 사이의 의미가 비슷한지 측정하는 모델을 가정.
이 모델은 2개의 입력(비교할 문장 2개)를 받아 0과 1사이의 점수를 출력.
0 -> 문장이 관련 없다!를 의미함.
1 -> 문장이 관련 있다!를 의미함.

이런 모델은 대화 시스템(dialog system)에서 
자연어 질의에 대한 중복 제거를 포함하여 많은 애플리케이션에서 유용하게 사용될 수 있음.

이 문제에서 두 입력 시퀀스가 바뀔 수 있음
why? 두 입력의 유사도는, 교환 법칙이 성립.

즉, 입력 문장을 처리하는 2개의 독립된 모델을 학습하는 것은 이치에 맞지 않음.
대신 하나의 LSTM 층으로 양쪽을 모두 처리하는 것이 좋음.

--> 샴 LSTM, 공유 LSTM 이라고 함.





















 




















