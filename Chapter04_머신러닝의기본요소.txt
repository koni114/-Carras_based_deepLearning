< Chapter04 _ 머신러닝의 기본 요소 >

** 4.1 머신 러닝의 네 가지 분류
일반적으로 머신 러닝 알고리즘은 다음 절에서 소개하는 4개의 커다란 범주 안에 속함
1. ** 지도 학습
가장 흔한 경우. 요즘 spotlight를 받는 광학 문자 판독. 음성 인식, 이미지 분류, 언어 번역과 같은
딥러닝의 거의 모든 애플리케이션이 일반적으로 이 범주에 속함
ex)  - 시퀀스 생성(sequence generation)
        사진이 주어지면, 사진을 설명하는 부수 설명을 예측! 
        어렵겠지..
      - 구문 트리 예측(syntax tree)
       문장이 주어지면 해당 구문 트리를 예측
        -> 프로그래밍 언어를 분석하는데 사용할 수 있음
      - 물체 감지(object detection)
        사진이 주어지면 사진 안에 특정 물체 주위에 boundingBox를 그림
      - 이미지 분할(image segmentation)
        이미지를 pixel 단위로 구분해 어떤 물체 class인지 구분하는 것을 말함

2. ** 비지도 학습
어떤 taget도 사용하지 않고 관심있는 데이터 변화를 찾음
데이터 시각화, 압축, 노이즈 제거 또는 상관관계를 더 잘 이해하기 위해 사용

차원 축소, 군집 분석이 대표적

3. ** 자기 지도 학습(self-supervised learning)
지도 학습의 특별한 경우지만, 별도의 범주로 구분하는 학습
지도학습이지만 사람이 만든 label을 사용하지 않음 
레이블이 필요한데, 레이블을 일반적으로 heuristic algorithm을 사용해서 입력 데이터로부터 생성함

ex) 오토인코더(autoencoder) : 입력을 출력으로 복사하는 것을 말함.
     지난 프레임이 주어졌을 때, 비디오의 다음 프레임을 예측하는 것
     이전 단어가 주어졌을 때, 다음 단어를 예측하는 것(이 경우 미래의 입력 데이터로부터 지도되기 때문에 시간에 따른 지도 학습임)

**  heuristic algorithm
문제에 있어 탐색작업을 축소시키기 위해 
항상 옳은 해를 제공하지는 못하지만 대부분의 경우에 잘맞는 
경험에 의한 규칙 (rules of thumb) 을 이용하는 algorithm을 의미
-> BFS, DFS와는 대비되는 algorithm
 

** 오토 인코더
입력 데이터를 가지고 하나의 코드로 압축한 뒤 요약된 코드로부터 입력 데이터를 재생성


지도/  학습(semi-supervised learning)
- 지도/비지도 학습을 섞어서 사용하는 것이라고 이해하면 편함
ex) MRI, CT 같은 의료 이미지가 지도/자율(semi supervised) 학습의 한 예.
    방사선 전문의가 몇 만장의 CT/MRI 자료를 보고 분류하기는 쉽지 않음.
    -> 딥러닝 신경망은 분류된 작은 부분으로도 이점을 취할 수 있고 완전한 자율 학습 모델보다
    정확성을 높일 수 있음


## -------------------------------------- ##

** 4.2.1 훈련,검증,테스트 세트
데이터를 훈련, 검증, 테스트 세트를 나누는 것은 간단하지만, 데이터가 적을 때는 몇가지
고급 기법을 이용해야 함. 대표적인 3가지 평가 방법


1. 단순 홀드아웃 검증(hold-out validation)
2. K-겹 교차 검증(K-fold corss-validation)
3. 셔플링(shuffling)을 사용한 반복 K-겹 교차 검증(iterated K-fold corss-validation)

** 단순 홀드아웃 검증
데이터의 일정량을 테스트 세트로 떼어 놓음
남은 데이터에서 훈련 후, 테스트 세트로 평가
당연히 테스트 데이터셋이 모델 형성에 관여해서는 절대 안됨!

-> 문제점 : 데이터 수가 작게 되면, 할 때마다 성능이 많이 달라질 수 있음
            이 문제를 해결하기 위하여 2,3번 검증 방법사용!


** K-겹 교차 검증(K-fold corss-validation)
데이터를 동일한 크기를 가진 K개 분할로 나눔
각 i 분할에 대해 K-1로 학습하고, i 분할로 검증 테스트 수행
총 K개의 검증 결과의 평균을 계산
모델 튜닝할 때 마다 각각 사용된 검증 세트를 사용하게 됨


** 셔플링(shuffling)을 사용한 반복 K-겹 교차 검증(iterated K-fold corss-validation)
비교적 가용 데이터가 적고, 정확하게 모델을 평가하고 싶을 때 사용
케글 경연에서 아주 크게 도움이 됨!
K-fold 교차 검증을 여러 번 사용하되, K개 분할로 나누기 전에 매번 데이터를 무작위로 섞는다
최종점수는 모든 K-fold 교차 검증을 실행해서 얻은 평균이 됨
P X K의 모델을 훈련하고 평가하므로 비용이 매우 많이 듬

sklearn 0.19버전에 추가된 RepeatedKFold(회귀)와 RepeatedStratifiedFold 클래스를
cross_validate 함수에 적용하여 구현할 수 있음

평가 방식을 선택할 때 기억해야 할 것!

하이퍼 파라미터 튜닝이 끝나면, 테스트 데이터를 제외한 모든 데이터들을 사용하여
모델을 다시 훈련시켜야 함

**대표성 있는 데이터 
훈련 데이터와 테스트 데이터는 전체 데이터에 대한 대표성을 가져야함
가장 조심해야 할 것은 데이터 데이터와 전체 데이터를 나눌 때, 전체 레이블의 비율이 
동일하게 들어가야 한다는 것

**시간의 방향
과거의 데이터로부터 미래를 예측하려고 할 때,
데이터를 분할 하기 전에 절대로 섞어서는 안됨. 그렇게 되면 미래 데이터가
모델링 과정에 포함되어 심각한 결과 오류를 발생시킬 수 있음

** 데이터 중복
훈련 데이터와 테스트 데이터에서 데이터 중복이 발생하게 되면
훈련 데이터로 테스트를 진행하는 것 처럼 되므로 이렇게 해서는 안됨!
따라서 반드시 중복 여부를 테스트 해보아야 함

## -------------------------------------- ##

4.3 데이터 전처리, feature engineering, feature learning
특성 공학 기법은 특정 도메인에 종속적(ex) 이미지 데이터, 텍스트 데이터에 특화)

** 신경망을 위한 데이터 전처리
주어진 원본 데이터를 신경망에 적용하기 쉽도록 만드는 것
벡터화(vectorization), 정규화(normalization), 누락된 값 다루기. 특성 추출 등이 포함

** 벡터화(vectorization)
사운드, 이미지, 텍스트 등 어떤 데이터든지, 텐서 형태로 변환해야 함
이를 "데이터 벡터화" 라고 함
ex) 2개의 텍스트 분류 예 
    텍스트 데이터 -> 정수 데이터 변환 -> float32 타입의 데이터로 이루어진 텐서로 변환
     "안녕하세요"     [2,3,7,10,1]         [1,0,1,1,0,1,1,0,0,0,0,.]

** 값 정규화
일반적으로 큰 값이나(가중치 값보다 훨씬 큰 값), 균일하지 않은 데이터(a 특성 : 0~1사이, b 특성 : 0~ 1000사이)
를 신경망에 주입하는 것은 매우 위험
이렇게되면 업데이트할 그레디언트가 커져 네트워크가 수렴하는 것을 방해
-> 평균이 0이고, 표준편차가 1이 되도록 정규화

** 결측 값이 있을 경우
신경망에서 0이 사전에 정의된 값이 아니라면,  0으로 입력해도 상관없음.
네트워크가 0이 누락된 값을 의미한다는 것을 학습하면 이 값을 무시하기 시작할 것임.

만약 테스트 데이터에서는 결측된 값이 없고, 테스트 데이터에서는 결측 값이 존재한다면,
누락된 값을 무시하는 방법을 모르므로, 반드시 train Dataset에 강제적으로 결측 값을 만들어서 모델링 수행

실제로 어떤 결측 처리 방법(0, mean, 이전 값 등..)이 주어진 문제에 적합한지는 알기 어렵.
-> 실제 교차 검증을 통해 확인해보는것이 중요.

결측 데이터의 행이나 열을 제거하는 것도 고려해볼만 함.

## -------------------------------------- ##         

4.3.2 특성 공학

feature engineering은 데이터와 머신러닝 알고리즘에 관한 지식을 사용하는 단계
모델에 데이터를 주입하기 전에 하드코딩된 변환을 적용하여 알고리즘이 더 잘 수행하도록 만들어줌
모델이 작업하기 수월한 형태로 데이터를 변환해주어야 할 필요성 존재

ex) 시계 이미지 데이터를 통해 시간을 알아내는 모델 구현.

단순히 이미지 픽셀 데이터를 딥러닝 모델을 구현하고 사용한다면 많은 컴퓨팅 파워가 사용됨
이를 특성 공학 측면에서 시계 이미지의 바늘 좌표를 추출하여 이를 특성으로 사용한다면,
머신 러닝이 전혀 필요하지 않게 됨

이런식의 접근은 해당 데이터와 목적을 정확하게 파악하고 있어야 함

최근 딥러닝은 이러한 특성 공학이 크게 중요하진 않음. 
딥러닝 모델 자체에서 데이터의 특성을 고려한 모델링을 내부적으로 만들어내기 때문


하지만 다음의 2가지 이유 때문에 특성 공학을 신경써야 함!

1. 좋은 특성은 적은 자원을 사용하여 문제를 멋지게 풀어낼 수 있음
2. 좋은 특성은 더 적은 데이터로 문제를 풀어낼 수 있음

## -------------------------------------- ##   

4.4 과대적합과 과소적합
머신러닝의 근본적인 이슈는 최적화와 일반화 사이의 줄다리기.
모델이 overfitting되지 않게 하려면, 가장 좋은 방법은 많은 훈련 데이터를 모으는 것!
-> 많은 훈련 데이터를 학습한 모델은 자연스레 일반화 특성을 많이 가지고 있음

차선책으로는 규제(regularization)를 하는 것. 다양한 규제 기법들을 알아야 함

** 네트워크 크기 축소
과대적합을 막는 가장 단순한 방법은, 파라미터 수(층 수, 유닛 수)를 줄이는 것
보통 파라미터의 수를 모델의 capacity(줄여서 capa라고 하겠지)라고 함

딥러닝 모델은 훈련 데이터에 잘 맞추려는 성향이 있음
-> 진짜 문제는 최적화가 아니고 일반화임을 기억하자.

결과적으로는 overfitting과 underfitting의 절충점을 잘 찾아야한다! (핵심)

알맞은 층 수, 유닛 수를 찾는 마법 같은 공식은 없음
알맞은 모델 크기를 찾으려면, 각기 다른 구조를 평가해 보아야 함

작은 유닛 수에서 시작하면서, 오버 피팅 지점을 찾아내는게 일반적.

** 가중치 규제 추가
오캄의 면도날(Occam's razor) 이론 
- 어떤 것에 대한 두 가지의 설명이 있다면, 더 적은 가정이 필요한 설명이 옳을 것이라는 이론
-> 딥러닝에도 적용되는데, 훈련데이터와 네트워크 구조가 주어졌을 때, 데이터를 설명할 수 있는
    가중치 값의 집합은 여러 개(여러 개의 모델)
    이에 간단한 모델이 복잡한 모델보다 덜 과대적합될 가능성이 높음..

간단한 모델은 파라미터 값 분포의 엔트로피가 작은 모델.

** 가중치 규제(weight regularization)
과대적합을 낮추기 위해서는 네트워크 복잡도의 제한을 두어 가중치가 작은 값을 가지도록 강제하는 것.
가중치 값의 분포가 더 균일하게 됨
 -> 당연. 예를 들어 가중치 값이 평균 1인데 한개가 50이라고 한다면, 이 값은 모델의 결과에 결과를 크게 미침. 
     이 값을 낮출 수 있는 제한을 두어 과대적합을 피할 수 있음. 이를 가중치 규제라고 함

L1 규제 : 가중치의 절대값에 비례하는 비용 추가. ( weight의 L1 norm)
L2 규제 : 가중치의 제곱에 비례하는 비용 추가
             weight의 L2 norm.  = 가중치 감쇠(weight decay라고도 함
                                       = 유클리디안 노름이라고도 부름

ex) kernel_regularizer = regularizers.l2(0.001) 
    -> 가중치 행렬의 모든 원소를 제곱하고 0.001을 곱하여 네트워크의 전체 손실에 더해진다는 의미

** L1, L2 규제 특징
L2 규제는 완전히 0이 되지는 않지만, L1은 0이 될 수 있음
L1, L2를 함께 쓰는 방식을 elasticNet이라고 함.
L1, L2 함수의 매개변수 기본값은 모두 0.01임

** 드롭아웃 추가
토론토 대학의 제프리 힌튼과 그의 학생들이 개발
네트워크 층에 드롭아웃을 적용하면 훈련하는 동안 무작위로 층의 일부 출력 특성을 제외(0으로 만듬)
[0.2, 0.5, 1.3, 0.8, 1.1] -> [0, 0.5, 1.3, 0, 1.1] 로 변경되어 출력.
드롭아웃의 비율은 0.2 ~ 0.5 사이로 측정
드롭아웃 비율은 0이 될 특성의 비율을 의미함
** 테스트 단계에서는 어떤 유닛도 drop 되지 않음(dropout 목적 자체가 모델의 overfitting을 방지하기 위함)
   -> 대신에 층의 출력을 드롭아웃 비율에 비례하여 줄여 줌 
** 훈련 단계에서 스케일을 낮추는 대신 역으로 늘려서 테스트 단계에서는 출력을 그대로 할 수도 있음!

드롭아웃은 은행의 부정 방지 매커니즘에서 착안
-> 은행의 행원들을 계속 순환시키면서 부정 행위를 방지함. 
    오랫동안 특정 위치에 행원들을 배치시켜두면 유대 관계가 생겨 부정 행위 발생 확률이 높음.

model.add(layers.Dropout(0, 5)) -> 추가하면 됨

신경망에서 과대적합을 방지하기 위한 방법
- 훈련 데이터를 더 모으기
- 네트워크의 용량을 감소
- 가중치 규제를 추가
- 드롭아웃 추가

## -------------------------------------- ##   

4.5 보편적인 머신 러닝 작업 흐름

1. 문제 정의와 데이터셋 수집

